{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding surveillance planes using random forests\n",
    "\n",
    "**The stories:**\n",
    "\n",
    "- https://www.buzzfeednews.com/article/peteraldhous/spies-in-the-skies\n",
    "- https://www.buzzfeednews.com/article/peteraldhous/hidden-spy-planes\n",
    "    \n",
    "This story, done by Peter Aldhous at Buzzfeed News, involved training a machine learning algorithm to recognize government surveillance planes based on what their flight patterns look like.\n",
    "\n",
    "**Datasets**\n",
    "\n",
    "* **feds.csv:** Transponder codes of planes operated by the federal government\n",
    "* **planes_features.csv:** various features describing each plane's flight patterns\n",
    "* **train.csv:** a labeled dataset of transponder codes and whether each plane is a surveillance plane or not\n",
    "    - The `label` column was originally `class`, but I renamed it because pandas freaks out a bit with a column named `class`\n",
    "    - This was created by Buzzfeed `feds.csv`\n",
    "* **data dictionary:** You can find the data dictionary published with their analysis [here](https://buzzfeednews.github.io/2016-04-federal-surveillance-planes/analysis.html)\n",
    "* **a few other files**\n",
    "\n",
    "## What's the goal?\n",
    "\n",
    "The FBI and Department of Homeland Security operate many planes that are not directly labeled as belonging to the government. If we can uncover these planes, we have a better idea of the surveillance activities they are undertaking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Let's also set a large number of maximum columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in our data\n",
    "\n",
    "Almost all classification problems start with a set of labeled features. In this case, the features are in one CSV file and the labels are in another.\n",
    "\n",
    "**Read in both `planes_features.csv` and `train.csv` and merge them on `adshex`, the transpoder code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No wait, merge them again!\n",
    "\n",
    "We have **features** for about 20,000 planes and **labels** for about 600 planes. This is because we don't know whether many of the planes are surveillance planes or not. When you merge, it only keeps rows you have **both features and labels for**.\n",
    "\n",
    "We want to keep those in the dataframe so we can play detective with them later, and try to find surveillance planes using the features. When you merge, you should use `how='left'` or `how='right'` to keep unmatched columns from the left (or right) dataframe.\n",
    "\n",
    "Save this merged dataframe as `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm you have 19,799 rows and 34 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up our data\n",
    "\n",
    "## Number-izing our labels\n",
    "\n",
    "Each row is a plane, and it's marked as either a surveillance plane or not. How many do we have in each category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you feel about that split? Is it balanced enough?\n",
    "\n",
    "**Let's this column for machine learning.** `\"surveil\"` and `\"other\"` won't work with sklearn because they're strings, not numbers. Adjust the `label` column to be something that we can use with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variables\n",
    "\n",
    "Do we have any variables that count as categories? Yes, we do, `type` of plane! **How many different categories does it have?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are **text**, not numbers, so we can't automatically use them in our classifier. But it's a little different than when we were working with actual documents - running this across a TF-IDF vetorizer shoudln't seem to make much sense.\n",
    "\n",
    "Instead, we'll just **make each plane type a number.** For example, `unknown` might be `0` and `C172` might be `1` and `SR22` might be `2`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to convert a list of categories into numbers, an easy way is to use the `Categorical` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.type = df.type.astype('category')\n",
    "df.type.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a normal bunch of strings, but pandas is secretly using a number for each one! You can find the number with `.cat.codes`.\n",
    "\n",
    "**We can use `df.type.cat.codes` to make a new columns called `type_code`.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type_code'] = df.type.cat.codes\n",
    "df[['type', 'type_code']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `type_code` for machine learning since sklearn needs a number, and `type` for reading since we like text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building our classifier\n",
    "\n",
    "To build a classifier, we need an `X` and a `y`. If we're working with text, we usually just use the `words_df` for the features and whatever our label column is for the `y`.\n",
    "\n",
    "```python\n",
    "X = words_df\n",
    "y = df.label_column\n",
    "```\n",
    "\n",
    "This time it's going to be a little bit different, and take a few more steps!\n",
    "\n",
    "First, since we have **labeled** and **unlabeled data** in the same dataframe, we need to get rid of all of the rows that don't have a `label`. Save this labeled dataset as `train_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm your `train_df` has 597 rows. **And while we're at it, let's look at the first five rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we make our `X` - our set of features that predicts the label - note that we also have a few extra columns that we aren't using to train our classifier:\n",
    "\n",
    "1. The `adshex` transponder code\n",
    "2. The text version of the plane type\n",
    "\n",
    "We'll need to get rid of these in a second!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your `X` and `y`.\n",
    "\n",
    "Creating your `X` and `y` for non-text-analysis projects looks different than for text-based analysis.\n",
    "\n",
    "In these non-text situations, we usually have a single dataframe that we want certain columns out of.\n",
    "\n",
    "* `X` are the columns that we use to make the predictions, our features. Usually we want every single column of data, which means we do `X = train_df.drop(columns=['label_column'])`. This makes sure we aren't including the label in our features.\n",
    "* `y` is the column that is the label that we are predicting\n",
    "\n",
    "In this case, though, we want to get rid of more than just the label column. Adjust the code below to also remove the transponder code and plane type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=['label'])\n",
    "y = train_df.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triple-check that `X` is a list of numeric features and and `y` is a numeric label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test datasets\n",
    "\n",
    "We could be nice and lazy and use all our data for training, but it just isn't right! Taking a test using the exact same questions you studied is just cheating. Split your data into test and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify using a logistic classifier\n",
    "\n",
    "## Train your classifier\n",
    "\n",
    "Build a `LogisticRegression` and fit it to your data, making sure you're training using only `X_train` and `y_train`.\n",
    "\n",
    "You can build a Logistic Regression classifier like this:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C=1e9, solver='lbfgs', max_iter=4000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining our classifier\n",
    "\n",
    "Let's use eli5 to explain our classifier. What are the important features for detecting a surveillance plane?\n",
    "\n",
    "```python\n",
    "import eli5\n",
    "\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "eli5.show_weights(clf, feature_names=feature_names)\n",
    "```\n",
    "\n",
    "Use the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How well does our classifier perform?\n",
    "\n",
    "Let's take a look at the confusion matrix to see how well this classifier finds surveillance planes. Make sure you're using `y_test` and `X_test`, not the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify using a decision tree\n",
    "\n",
    "Now we'll use a decision tree. This is how you make one:\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "```\n",
    "\n",
    "If we use `max_depth=` to limit the depth of the tree, it will help us visualize it. For example, `max_depth=5` will only allow the tree to make five decisions.\n",
    "\n",
    "Make a decision tree and fit it to your data. Use a `max_depth=` of something between 2 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the important features?\n",
    "\n",
    "We'll use slighyl different code for a decision tree, as it likes to draw big pictures if we don't stop it. The code looks like this:\n",
    "\n",
    "```python\n",
    "import eli5\n",
    "\n",
    "feature_names=list(X.columns)\n",
    "eli5.show_weights(clf, feature_names=feature_names, show=['description', 'feature_importances'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the output\n",
    "\n",
    "We'll do this in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How well does the tree perform?\n",
    "\n",
    "Display another confusion matrix with your new classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the tree\n",
    "\n",
    "You can use `eli5` to visualize the decision tree itself! It usually takes up too much space, but since it's a special occasion we'll let it go. You... might need to install something for this to work? I'm not 100% sure, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=list(X.columns)\n",
    "\n",
    "label_names = ['not surveillance', 'surveillance']\n",
    "eli5.show_weights(clf, feature_names=feature_names, target_names=label_names, show=['decision_tree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like your graph to have colors colors, or to not use eli5, you can do it the old-fashioned way. You might need to `brew install graphviz` and `pip install graphviz`. Windows users will need to download and install from [the graphviz website](https://graphviz.org/download/), and potentially add graphviz to their path.\n",
    "\n",
    "```python\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "label_names = ['not surveillance', 'surveillance']\n",
    "feature_names = X.columns\n",
    "\n",
    "dot_data = tree.export_graphviz(clf,\n",
    "                    feature_names=feature_names,  \n",
    "                    filled=True,\n",
    "                    class_names=label_names)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph\n",
    "```\n",
    "\n",
    "* **Tip:** You'll probably need to scroll sideways a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One more classifier: Random forest\n",
    "\n",
    "## Build and train your classifier\n",
    "\n",
    "We can build a random forest classifier like this:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "```\n",
    "\n",
    "But you're in charge of fitting it to your training data!\n",
    "\n",
    "* **Tip:** You can also set `max_depth` here if you want.\n",
    "* **Tip:** Increase `n_estimators` to 100 to make a better classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the important features?\n",
    "\n",
    "Use eli5 to obtain the feature importances. The best method for a random forest is below.\n",
    "\n",
    "```python\n",
    "feature_names = list(X.columns)\n",
    "eli5.show_weights(clf, feature_names=feature_names, show=['description', 'feature_importances'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the output\n",
    "\n",
    "Random forests are a collection of decision trees. We'll talk about how to understand the feature importance in class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How well does it perform?\n",
    "\n",
    "Use a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which model are you most confident in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actually finding spy planes\n",
    "\n",
    "Now let's try ot actually find our spy planes\n",
    "\n",
    "## Retrain our model\n",
    "\n",
    "When we did test/train split, we trained our model with only a subset of our data, so we could test with the rest. Now that we're working in the \"real world\" we want to re-train it using not just `_train` and `_test` data, but instead **everything we have labels for.**\n",
    "\n",
    "That's the whole `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter for planes we want to predict\n",
    "\n",
    "We have a dataframe of features that includes three types of planes:\n",
    "\n",
    "* Those that are labeled as surveillance planes\n",
    "* Those that are labeled as not surveillance\n",
    "* Those that aren't labeled\n",
    "\n",
    "Which do we want to predictions for? **Filter to create new dataframe that's just unlabeled planes.** We'll call it `unknown_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many planes do you have in that list? **Confirm it's about 19,200.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting \n",
    "\n",
    "Build your `X_unknown` - remember you need to drop a few columns! We only want **numeric features** here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use that to make a prediction for each plane, and **assign the prediction into the `predicted` column of `unknown_df`**.\n",
    "\n",
    "* **Tip:** Scroll up to see where you created your features for training, it's similar\n",
    "* **Tip:** pandas will yell at us about setting values on copies of a slice but it's fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many planes did it predict to be surveillance planes?\n",
    "\n",
    "It should be roughly around 70-80 planes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But.. what about those other ones? The ones that are just below the threshold?\n",
    "\n",
    "The cutoff for a prediction of `1` is 50%, but since we have a lot of time we're interested in investigating the top 150.\n",
    "\n",
    "To get the probability for each row, you will use `clf.predict_proba` instead of `clf.predict`. Also, to get the predicted probability for the `1` category, you'll need to add `[:,1]`, which gives us _something like_ this:\n",
    "\n",
    "```python\n",
    "clf.predict_proba(unknown_df.drop(columns=['label', 'adshex', 'type']))[:,1]\n",
    "```\n",
    "\n",
    "**Create a new column called `predicted_prob` that is the chance that the plane is a surveillance plane.**\n",
    "\n",
    "* **Tip:** You dropped three columns when using `clf.predict`, but if you drop the same three (e.g. you cut and pasted the code above) you'll get an error now. There's now an extra column that you'll need to drop! What is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the top 200 predictions\n",
    "\n",
    "Take a look at what the probabilities look like, showing the top 200 planes that are **most likely to be surveillance planes.**\n",
    "\n",
    "Then save them to a file for later research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save these top 200 to a CSV named `planes-to-research.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using words and not column names, describe what the machine learning algorithm found to be important when identifying surveillance planes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did we use test/train split when it would have been more effective to give our model all of the data from the start?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did we use a random forest instead of a decision tree or logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did we use probability instead of just looking for planes with a predicted value of 1? It seems like we should have just trusted the algorithm, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The government could claim that we're threatening national security by publishing this paper as well as publishing this code - now anyone could look for planes that are surveilling them. What do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using data from the past, but you can get real-time flight data from many services. Can you think of any uses for this algorithm using real-time instead of historical data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
